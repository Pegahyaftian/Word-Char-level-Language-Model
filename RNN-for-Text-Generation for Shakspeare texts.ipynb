{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2d6735",
   "metadata": {},
   "source": [
    "# RNN-for-Text-Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6704ba",
   "metadata": {},
   "source": [
    "Text generation (encoded variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec867d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202b841",
   "metadata": {},
   "source": [
    "## Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_characters = set(text)\n",
    "unique_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6889ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aff5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder ={}\n",
    "for k,v in decoder.items():\n",
    "    encoder[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d994cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf68118",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "length = len(set(encoded_text))\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20617b5",
   "metadata": {},
   "source": [
    "# One-hot Encoding\n",
    "The data is needed to be one hot encoded to be capable of feeding into the NN structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    \n",
    "    # Create a placeholder for zeros\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61155430",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder(np.array([1,2,0]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be364a5",
   "metadata": {},
   "source": [
    "# Creating Training Batches\n",
    "We need to create a function that will generate batches of characters along with the next character in the sequence as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acab9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "        \n",
    "        while True:    \n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2102f",
   "metadata": {},
   "source": [
    "### Example of generating a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = np.arange(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f988c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3765cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text,samp_per_batch=4,seq_len=5)\n",
    "batch_genertor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56354a",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03733773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden = 256, num_layers= 4,drop_prob=0.5, use_gpu=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        \n",
    "        #Character set, Encoder and Decoder\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:idx for idx,char in self.decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=len(self.all_chars),hidden_size=num_hidden,num_layers=num_layers,dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x,hidden)\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = dropoutput.reshape(-1,self.num_hidden)\n",
    "        output = self.fc_linear(drop_output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_state(self,batch_size=128):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cude'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            \n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                 torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62179311",
   "metadata": {},
   "source": [
    "## Instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8839de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMmodel(all_chars = unique_characters, num_hidden =512,num_layers= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ad889",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param=[]\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))\n",
    "    \n",
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41808bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695bd79",
   "metadata": {},
   "source": [
    "## Optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ab144",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c596822",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(encoded_text) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:idx]\n",
    "val_data = encoded_text[idx:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13c08b",
   "metadata": {},
   "source": [
    "# Training Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87952a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "##variables \n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "seq_len = 100\n",
    "num_char = max(encoded_text)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model to train\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "model.train()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(128)\n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "        i += 1\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        #convert numpy array to tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        targets = torch.from_numpy(y).to(device)\n",
    "        \n",
    "        #reset hidden state after each batch since batches are cosidered independant\n",
    "        hidden = tuple([state.detach() for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(bathc_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        #CLIP for tackle gradient exploding\n",
    "        nn.utils.clip_grad_norm(model.parameters())\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        ##########################\n",
    "        ### validation set\n",
    "        if i % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data, batch_size, seq_len):\n",
    "                \n",
    "                x = one_hot_encoder(x, num_char)\n",
    "                \n",
    "                #convert Numpy arrays to Tensor\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                \n",
    "                val_hidden = tuple([state.detach() for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forwrd(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len))\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                print(f\" Epoch :{epoch} ,Step :{i} ,Val Loss:{val_loss.item()} \")\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5812f",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d96323",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.dict_state(),'model_Shakspeare.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a966d0",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_Shakspeare.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a73216",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=unique_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb43e1",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a138418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model,char, hidden=None,k=1):\n",
    "    \n",
    "    #Encode raw letters with model\n",
    "    encoded_text = model.encoder[char]\n",
    "    \n",
    "    #Need numpy array for one-hot encoding\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    \n",
    "    #One-hot encoding\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "    \n",
    "    #Convert to Tensor\n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "    #detach hidden states\n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    #Run model and get predictions\n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "    \n",
    "    probs = F.softmax(lstm_out,dim = 1).detach().cpu()\n",
    "    top_k , idxs = probs.topk(k)\n",
    "    \n",
    "    idxs = idxs.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    \n",
    "    char = np.random.choice(index_positions, p=probs/probs.sum())\n",
    "    \n",
    "    return model.decoder[char], hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69517517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed=\"The\", k = 1):\n",
    "    \n",
    "    if cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    output_chars = [s for s in seed]\n",
    "    hidden = model.hidden_state(1)\n",
    "\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model,char,hidden, k=k)\n",
    "    \n",
    "    output_chars.append(char)\n",
    "    \n",
    "    \n",
    "    for i in range(size):\n",
    "        \n",
    "        char, hidden = predict_next_char(model,output_chars[-1], hidden, k=k)\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16721f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9f219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
